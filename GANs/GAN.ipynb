{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i replicate the paper of 'Generative Adversarial Nets' from 2014, to generate handwritten numbers using the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/1875                       Loss D: 0.7407, loss G: 0.7217\n",
      "Epoch [1/50] Batch 0/1875                       Loss D: 0.8602, loss G: 0.6461\n",
      "Epoch [2/50] Batch 0/1875                       Loss D: 0.4275, loss G: 1.4299\n",
      "Epoch [3/50] Batch 0/1875                       Loss D: 0.2308, loss G: 1.9767\n",
      "Epoch [4/50] Batch 0/1875                       Loss D: 0.6046, loss G: 0.8696\n",
      "Epoch [5/50] Batch 0/1875                       Loss D: 0.6486, loss G: 0.9889\n",
      "Epoch [6/50] Batch 0/1875                       Loss D: 0.8064, loss G: 0.9886\n",
      "Epoch [7/50] Batch 0/1875                       Loss D: 1.0574, loss G: 0.5108\n",
      "Epoch [8/50] Batch 0/1875                       Loss D: 0.6067, loss G: 0.9279\n",
      "Epoch [9/50] Batch 0/1875                       Loss D: 0.4648, loss G: 1.3461\n",
      "Epoch [10/50] Batch 0/1875                       Loss D: 0.4855, loss G: 1.7373\n",
      "Epoch [11/50] Batch 0/1875                       Loss D: 0.5832, loss G: 1.3158\n",
      "Epoch [12/50] Batch 0/1875                       Loss D: 0.6395, loss G: 1.0343\n",
      "Epoch [13/50] Batch 0/1875                       Loss D: 0.8506, loss G: 1.1053\n",
      "Epoch [14/50] Batch 0/1875                       Loss D: 0.7396, loss G: 1.2383\n",
      "Epoch [15/50] Batch 0/1875                       Loss D: 0.3915, loss G: 2.0209\n",
      "Epoch [16/50] Batch 0/1875                       Loss D: 0.4764, loss G: 1.5155\n",
      "Epoch [17/50] Batch 0/1875                       Loss D: 0.8993, loss G: 1.0147\n",
      "Epoch [18/50] Batch 0/1875                       Loss D: 0.4840, loss G: 1.4831\n",
      "Epoch [19/50] Batch 0/1875                       Loss D: 0.7446, loss G: 1.5022\n",
      "Epoch [20/50] Batch 0/1875                       Loss D: 0.5922, loss G: 1.3985\n",
      "Epoch [21/50] Batch 0/1875                       Loss D: 0.9023, loss G: 0.9280\n",
      "Epoch [22/50] Batch 0/1875                       Loss D: 0.7233, loss G: 1.4390\n",
      "Epoch [23/50] Batch 0/1875                       Loss D: 0.4582, loss G: 1.5829\n",
      "Epoch [24/50] Batch 0/1875                       Loss D: 0.6442, loss G: 1.0672\n",
      "Epoch [25/50] Batch 0/1875                       Loss D: 0.6283, loss G: 1.1816\n",
      "Epoch [26/50] Batch 0/1875                       Loss D: 0.4024, loss G: 1.6127\n",
      "Epoch [27/50] Batch 0/1875                       Loss D: 0.6848, loss G: 1.4033\n",
      "Epoch [28/50] Batch 0/1875                       Loss D: 0.5164, loss G: 1.2815\n",
      "Epoch [29/50] Batch 0/1875                       Loss D: 0.5481, loss G: 1.7868\n",
      "Epoch [30/50] Batch 0/1875                       Loss D: 0.5665, loss G: 1.7613\n",
      "Epoch [31/50] Batch 0/1875                       Loss D: 0.6366, loss G: 1.2069\n",
      "Epoch [32/50] Batch 0/1875                       Loss D: 0.4953, loss G: 1.6302\n",
      "Epoch [33/50] Batch 0/1875                       Loss D: 0.4311, loss G: 1.2320\n",
      "Epoch [34/50] Batch 0/1875                       Loss D: 0.6666, loss G: 1.3695\n",
      "Epoch [35/50] Batch 0/1875                       Loss D: 0.5482, loss G: 1.5125\n",
      "Epoch [36/50] Batch 0/1875                       Loss D: 0.5007, loss G: 1.1949\n",
      "Epoch [37/50] Batch 0/1875                       Loss D: 0.5266, loss G: 1.4740\n",
      "Epoch [38/50] Batch 0/1875                       Loss D: 0.6724, loss G: 1.2418\n",
      "Epoch [39/50] Batch 0/1875                       Loss D: 0.4744, loss G: 1.3207\n",
      "Epoch [40/50] Batch 0/1875                       Loss D: 0.4195, loss G: 1.4619\n",
      "Epoch [41/50] Batch 0/1875                       Loss D: 0.7263, loss G: 1.0709\n",
      "Epoch [42/50] Batch 0/1875                       Loss D: 0.5973, loss G: 1.2087\n",
      "Epoch [43/50] Batch 0/1875                       Loss D: 0.7568, loss G: 1.1158\n",
      "Epoch [44/50] Batch 0/1875                       Loss D: 0.7038, loss G: 1.3626\n",
      "Epoch [45/50] Batch 0/1875                       Loss D: 0.4303, loss G: 1.5126\n",
      "Epoch [46/50] Batch 0/1875                       Loss D: 0.6173, loss G: 1.2186\n",
      "Epoch [47/50] Batch 0/1875                       Loss D: 0.6282, loss G: 0.8263\n",
      "Epoch [48/50] Batch 0/1875                       Loss D: 0.6187, loss G: 1.3679\n",
      "Epoch [49/50] Batch 0/1875                       Loss D: 0.5276, loss G: 1.2494\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Simple GAN using fully connected layers\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "\n",
    "# Building the discriminator nn\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1), # the output is a single value (real or fake)\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "# Building the generator nn\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256), # z_dim is the dimention of the \"latent noise\"\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "device =\"mps\" # \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64 # z_dim is the dimention of the \"latent noise\"\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device) # fixed noise that will be use to watch the progress of the generator in a batch in Tensorboard.\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.MNIST(root=\"dataset_GAN/\", transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss() # loss = - Wn[yn * log(xn) + (1 - yn)*log(1-xn)]\n",
    "\n",
    "writer_fake = SummaryWriter(log_dir= f\"runs/GAN_MNIST/fake\")\n",
    "writer_real = SummaryWriter(log_dir= f\"runs/GAN_MNIST/real\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader): # here we do not use the labels of the MNIST dataset, GAN are unsupervised\n",
    "        \n",
    "        real = real.view(-1, 784).to(device) #flatten the img\n",
    "        batch_size = real.shape[0]\n",
    "        \n",
    "        #### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device) # generating the random noise\n",
    "        fake = gen(noise) # generating fake images\n",
    "    \n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real)) # D(real)\n",
    "        # if yn = 1 (torch.ones_like) --> loss = - 1*[1 * log(disc(real))]\n",
    "        \n",
    "        disc_fake = disc(fake).view(-1) \n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) # min log(D(G(z)))\n",
    "        # if yn = 0 (torch.zeros_like) --> loss = - 1*[(1 - 0) * log(1 - disc(fake))]\n",
    "        \n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        \n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "\n",
    "        #### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = disc(fake).view(-1) \n",
    "        lossG = criterion(output, torch.ones_like(output)) # max log(D(G(z))\n",
    "        \n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28) # we use our gen in a fixed_noise batch, to see the progress after each epoch in tensorboard.\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to improve the GAN using a better normalization with batchnorm and adding more layers to our nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "\n",
    "# Building the discriminator nn\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(1024, 1), # the output is a single value (real or fake)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "      \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "# Building the generator nn\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256), # z_dim is the dimention of the \"latent noise\"\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 550),\n",
    "            nn.BatchNorm1d(550),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(550, img_dim),\n",
    "            nn.BatchNorm1d(img_dim),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Tanh()  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "device =\"mps\" # \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.MNIST(root=\"dataset_GAN/\", transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss() # loss = - Wn[yn * log(xn) + (1 - yn)*log(1-xn)]\n",
    "\n",
    "writer_fake = SummaryWriter(log_dir= f\"runs/GAN_MNIST/fake_2\")\n",
    "writer_real = SummaryWriter(log_dir= f\"runs/GAN_MNIST/real_2\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader): # here we do not use the labels of the MNIST dataset, GAN are unsupervised\n",
    "        \n",
    "        real = real.view(-1, 784).to(device) #flatten the img\n",
    "        batch_size = real.shape[0]\n",
    "        \n",
    "        #### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device) # generating the random noise\n",
    "        fake = gen(noise) # generating fake images\n",
    "    \n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real)) # D(real)\n",
    "        # if yn = 1 (torch.ones_like) --> loss = - 1*[1 * log(disc(real))]\n",
    "        \n",
    "        disc_fake = disc(fake).view(-1) \n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) # min log(D(G(z)))\n",
    "        # if yn = 0 (torch.zeros_like) --> loss = - 1*[(1 - 0) * log(1 - disc(fake))]\n",
    "        \n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        \n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "\n",
    "        #### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = disc(fake).view(-1) \n",
    "        lossG = criterion(output, torch.ones_like(output)) # max log(D(G(z))\n",
    "        \n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will try changing the architecture to a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "466813897a52447f1831c184b0700fb7b7f042a70becf90568dd3beaa877b3ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

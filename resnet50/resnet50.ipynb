{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rSs52SOmtAO"
      },
      "source": [
        "Here im doing transfer learning and fine tunning with a resnet50 and pytorch, to classify bees and ants"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title colab sincronization and dict of labels\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "label_dic = {0:'Ant',\n",
        "             1:'Bee'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkliWcyTFL5c",
        "outputId": "c1e67633-77b8-4fdf-a977-87048555f44a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, dataset\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fbkaC-XhtaUe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First we have to find the means and stds of our train dataset to lately Normalize\n",
        "\n",
        "# Define a transform to convert images to tensors\n",
        "to_tensor = transforms.ToTensor()\n",
        "\n",
        "# Initialize lists to store the tensors and the total number of tensors\n",
        "tensors = []\n",
        "total_tensors = 0\n",
        "directories = ['/content/drive/MyDrive/Colab Notebooks/resnet/data_resnet50/train/bees',\n",
        "               '/content/drive/MyDrive/Colab Notebooks/resnet/data_resnet50/train/ants']\n",
        "# Iterate over the directories\n",
        "for directory in directories:\n",
        "    # Load all the images in the directory\n",
        "    images = []\n",
        "    for file in os.listdir(directory):\n",
        "        if file.endswith('.jpg'):\n",
        "            image = Image.open(os.path.join(directory, file))\n",
        "            images.append(image)\n",
        "\n",
        "    # Convert the images to tensors and add them to the list\n",
        "    tensors.extend([to_tensor(image) for image in images])\n",
        "    total_tensors += len(images)\n",
        "\n",
        "# Calculate the mean and std of the tensors for each channel\n",
        "means_data = []\n",
        "stds_data = []\n",
        "for i in range(3):  # 3 channels (R, G, B)\n",
        "    mean = 0.\n",
        "    std = 0.\n",
        "    for tensor in tensors:\n",
        "        mean += tensor[i, :, :].mean()\n",
        "        std += tensor[i, :, :].std()\n",
        "    mean /= total_tensors\n",
        "    std /= total_tensors\n",
        "    means_data.append(mean)\n",
        "    stds_data.append(std)\n",
        "\n"
      ],
      "metadata": {
        "id": "laiJhyYYr6OY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acMukUmR8vul",
        "outputId": "19cff005-d7d7-4ce9-cde5-e0e57fde82ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ants', 'bees']\n"
          ]
        }
      ],
      "source": [
        "# Define a transform to preprocess the input images\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means_data, stds_data) # Normalize the pixel values\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means_data, stds_data) # Normalize the pixel values\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Import Data\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/resnet/data_resnet50'\n",
        "sets = ['train', 'validation']\n",
        "image_datasets = {x:datasets.ImageFolder(os.path.join(data_dir, x), \n",
        "                                         data_transforms[x])\n",
        "                  for x in ['train', 'validation']}\n",
        "\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size = 4,\n",
        "                             shuffle = True)\n",
        "               for x in ['train', 'validation']}\n",
        "\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation']}\n",
        "class_names = image_datasets['train'].classes\n",
        "print(class_names)\n",
        "\n",
        "# Training \n",
        "def train_model(model, criterion, optimazer, scheduler, num_epochs = 10):\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "        \n",
        "        #Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train() # set the model in training mode\n",
        "            else:\n",
        "                model.eval()  # set the model in evaluation mode\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "                \n",
        "                \n",
        "            # iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                #forward\n",
        "                #track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                #backward + optimize, only if is training phase\n",
        "                if phase == 'train':\n",
        "                    # Clean the gradients of the parameters to optimize. \n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step() # Updating the weights\n",
        "            \n",
        "            #statistics\n",
        "            running_loss +=loss.item()* inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "        if phase == 'train':\n",
        "            scheduler.step()\n",
        "        \n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double()/ dataset_sizes[phase]\n",
        "        \n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "        \n",
        "        #deep copy the model \n",
        "        if phase == 'validation' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    print(f'Training Complete, Best Val Acc: {best_acc:.4f}')\n",
        "    \n",
        "    # Load best model Weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsoLoUSg8vup",
        "outputId": "470dc53a-1f2f-44d0-be92-9e09097dfc4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10\n",
            "----------\n",
            "validation Loss: 0.0000 Acc: 0.0067\n",
            "Epoch 1/10\n",
            "----------\n",
            "validation Loss: 0.0304 Acc: 0.0000\n",
            "Epoch 2/10\n",
            "----------\n",
            "validation Loss: 0.0189 Acc: 0.0000\n",
            "Epoch 3/10\n",
            "----------\n",
            "validation Loss: 0.0000 Acc: 0.0067\n",
            "Epoch 4/10\n",
            "----------\n",
            "validation Loss: 0.0000 Acc: 0.0067\n",
            "Epoch 5/10\n",
            "----------\n",
            "validation Loss: 0.0000 Acc: 0.0067\n",
            "Epoch 6/10\n",
            "----------\n",
            "validation Loss: 0.0000 Acc: 0.0067\n",
            "Epoch 7/10\n",
            "----------\n",
            "validation Loss: 0.0000 Acc: 0.0067\n",
            "Epoch 8/10\n",
            "----------\n",
            "validation Loss: 0.0000 Acc: 0.0067\n",
            "Epoch 9/10\n",
            "----------\n",
            "validation Loss: 0.0000 Acc: 0.0067\n",
            "Epoch 10/10\n",
            "----------\n",
            "validation Loss: 0.0000 Acc: 0.0067\n",
            "\n",
            "Training Complete, Best Val Acc: 0.0067\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet50(pretrained = True)\n",
        "\n",
        "# Now we will freeze the first layers of the NN, to train the weights only \n",
        "# of the latest layer\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# The resnet50 was trained on ImageNet dataset, that's why it has 1000 labels in \n",
        "# the output layer (out_features), in our implementation, we want to classify only \n",
        "# 2 labels(bees & ants, so, we will edit the last layer of the resnet.\n",
        "\n",
        "num_classes = 2\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "\n",
        "# model to GPU\n",
        "device = torch.device(\"cuda:0\") \n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Define a loss function and an optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "# optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
        "# optimizer = torch.optim.Adadelta(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# Every 7 epochs our learning rate its only update 10%.\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma = 0.1)\n",
        "\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs= 11)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTIONAL: SAVING THE WEIGHTS"
      ],
      "metadata": {
        "id": "Kcf_xn7M6Jfj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DwHqbzsRBL8A"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"./resnet50_finetuned.pth\") # saving the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AlYMDqctLazA"
      },
      "outputs": [],
      "source": [
        "# model_loaded = models.resnet50(pretrained = True) # Loading the weights that i saved before\n",
        "\n",
        "# num_classes = 2\n",
        "# num_ftrs = model_loaded.fc.in_features\n",
        "# model_loaded.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# model_loaded.load_state_dict(torch.load(\"./resnet50_finetuned.pth\"))\n",
        "# model_loaded.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XS4figvt8vur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "039e013f-0e3a-4e95-e012-d663eeafac8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ant'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# To resize and crop the image, you can use the Resize and CenterCrop transforms\n",
        "# To convert the image to a tensor and normalize its pixel values, you can use \n",
        "# the ToTensor and Normalize \n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(means_data, stds_data)\n",
        "])\n",
        "\n",
        "\n",
        "X_test = Image.open('/content/drive/MyDrive/Colab Notebooks/resnet/test_resnet50/hormiga1.jpg')\n",
        "image_tensor = transform_test(X_test)\n",
        "image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "image_tensor = image_tensor.to(device)\n",
        "\n",
        "y_pred = model(image_tensor)\n",
        "_, pred = torch.max(y_pred, 1)\n",
        "label_dic[pred.item()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = Image.open('/content/drive/MyDrive/Colab Notebooks/resnet/test_resnet50/hormiga2.jpg')\n",
        "image_tensor = transform_test(X_test)\n",
        "image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "image_tensor = image_tensor.to(device)\n",
        "\n",
        "y_pred = model(image_tensor)\n",
        "_, pred = torch.max(y_pred, 1)\n",
        "label_dic[pred.item()]"
      ],
      "metadata": {
        "id": "WyUdFftrXK6v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2a606fb5-c4ec-4a8a-ead4-5871fede61cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ant'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = Image.open('/content/drive/MyDrive/Colab Notebooks/resnet/test_resnet50/aveja1.jpg')\n",
        "image_tensor = transform_test(X_test)\n",
        "image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "image_tensor = image_tensor.to(device)\n",
        "\n",
        "y_pred = model(image_tensor)\n",
        "_, pred = torch.max(y_pred, 1)\n",
        "label_dic[pred.item()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "X8Ob14KW7yNd",
        "outputId": "76e47b28-ac8c-40d2-9334-b9c2f2df047e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = Image.open('/content/drive/MyDrive/Colab Notebooks/resnet/test_resnet50/aveja2.jpg')\n",
        "image_tensor = transform_test(X_test)\n",
        "image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "image_tensor = image_tensor.to(device)\n",
        "\n",
        "y_pred = model(image_tensor)\n",
        "_, pred = torch.max(y_pred, 1)\n",
        "label_dic[pred.item()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vJ6Hvffx7yo0",
        "outputId": "b407de73-376a-4fe5-a4ea-5dbb355d653a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = Image.open('/content/drive/MyDrive/Colab Notebooks/resnet/test_resnet50/amarillo_negro.jpg')\n",
        "image_tensor = transform_test(X_test)\n",
        "image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "image_tensor = image_tensor.to(device)\n",
        "\n",
        "y_pred = model(image_tensor)\n",
        "_, pred = torch.max(y_pred, 1)\n",
        "label_dic[pred.item()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cZ_-bOzQGlSC",
        "outputId": "b5f029cf-9f4d-40de-f520-1e6d0647554d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rHpbFiz0H6w2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.8 ('redes')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "466813897a52447f1831c184b0700fb7b7f042a70becf90568dd3beaa877b3ca"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}